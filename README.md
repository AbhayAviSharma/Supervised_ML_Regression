# Supervised_ML_Regression
The primary objective of this project is to develop a robust Machine Learning Regression model that can accurately forecast YES Bank's monthly closing Stock prices using past data . We have used "Python" libraries like "Pandas", "NumPy" for EDA, libraries like "Matplotlib", "Seaborn" for visualization and "Sci-kit learn" library for modelÂ building.

![stocks](https://github.com/AbhayAviSharma/Supervised_ML_Regression/assets/131509148/152e1674-408d-42b4-ae45-27f3b825e204)

# Summary :
Yes Bank, a prominent institution in the Indian financial sector, has been in the spotlight since 2018 due to a high-profile fraud case involving its former CEO, Rana Kapoor. This project aims to explore the relationship between various financial indicators and Yes Bank's monthly closing stock prices. By employing regression analysis, we seek to develop a predictive model that can accurately forecast the bank's closing stock prices. The dataset used in this project includes historical monthly stock prices and a set of financial indicators.

Steps performed:

![download1](https://github.com/AbhayAviSharma/Supervised_ML_Regression/assets/131509148/155ddb6b-7a86-4d7b-aff4-ae4bb1b63a4e)

1. Data Collection and Preprocessing: Gathering historical monthly closing stock prices of Yes Bank, along with financial indicators such as earnings, interest rates, and economic conditions, and preprocessing the dataset to ensure its quality and integrity.
2. Feature Selection: Identifying and selecting the most influential financial indicators that have a significant impact on Yes Bank's stock prices.
3. Regression Modeling: Developing an effective regression model (e.g., Linear Regression, Ridge Regression, Lasso Regression, or Random Forest Regression) that can capture the relationships between the selected features and Yes Bank's closing stock prices.
4. Model Evaluation: Assessing the performance of the regression model(s) using appropriate evaluation metrics (e.g., Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, R-squared) to determine its predictive accuracy and reliability.
5. Model Fine-Tuning: If necessary, fine-tuning the selected regression model(s) to enhance predictive performance.

# Conclusion:
Our evaluation encompassed several key regression techniques, including Linear Regression, Lasso Regression, Ridge Regression, ElasticNet Regression, Decision Tree Regression, Random Forest Regression, K-Neighbors Regressor, and XGBoost Regressor. Each of these models was rigorously assessed using a range of performance metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), R2 Score, and Adjusted R2 Score.

Among the models considered, Linear Regression stood out as the top-performing candidate for our predictive task. It demonstrated exceptional accuracy with the lowest MAE, MSE, RMSE and MAPE values, signifying precise predictions. Additionally, the model exhibited a high R2 and adjusted R2 Score, indicating its ability to explain a substantial portion of the variance in the target variable. Linear Regression also offered the valuable advantage of feature selection, enhancing model interpretability by automatically identifying and prioritizing important predictors.

In conclusion, this project's journey through various regression models has provided valuable insights into the predictive modeling process. The selection of Linear Regression as the preferred model represents a significant milestone in achieving accurate and interpretable predictions for predicting closing stock price for Yes bank dataset. This project not only delivers a powerful predictive tool but also underscores the importance of careful model selection and evaluation in machine learning endeavors.
